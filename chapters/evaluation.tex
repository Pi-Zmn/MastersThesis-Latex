\chapter{Evaluation}
\label{ch:evaluation}
This chapter presents the evaluation of the implemented WebCrowd platform through a series of experiments designed to address the research questions defined in \autoref{subsec:into:objectives:questions}. These three research questions focus on WebCrowds computational capabilities when handling complex parallelizable tasks, its dynamic viability in managing fluctuating worker participation, and its ability to support heterogeneous devices as workers. Each of the following sections in this chapter corresponds to one of the research questions and presents the experimental setup, the expected outcome, and a analysis of these results.

The experiments of the evaluation utilize the implemented visualization of the Mandelbrot set, described in \autoref{sec:implementation:benchmark}, as a benchmark job. This benchmark job represents a computationally intensive test case, which can be used to demonstrate and test WebCrowd's capabilities. Each task of this benchmark job coveres a unique 1500x1500 area of the Mandelbrot set and all generated \ac{PNG} files have the same resolution. However, the computation time varies significantly among each of these tasks. According to the Mandelbrot function in (\ref{equ:mandelbrot}) require complex numberers that are part of the Mandelbrot set more iterations of the calculation than complex numbers that are not part of the Mandelbrot set. Therefore depends the execution time of a task on the amount of calculated points that belong to the Mandelbrot set.

Additionally, \autoref{sec:evaluation:languages} compares the performance of the various in WebCrowd implemented WebAssembly environments, each supporting the execution of source code from different programming languages compiled to a WebAssembly binary.

\section{Computational Capability}
\textbf{Is WebCrowd capable of successfully solving large, parallelizable problems?} 
\newline
The objective of the following experiment is to evaluate WebCrowd's ability to successfully execute computationally intensive, parallelizable tasks across a distributed network of volunteer workers. This empirical experiment compares the total execution time between distributed computation across multiple workers and native execution on a single computer.

\subsubsection{Experimental Setup}
At first the benchmark job was computed on the system specified in \autoref{app:system:server} in a native Go environment using the source code of \autoref{app:code:mandelbrot3}. The resulting computation time serves as a baseline for the following experiments, because this system is later also used to host the WebCrowd platform. Since this system is already required to serve WebCrowd in the first place, the following experiments additionally investigate whether distributing the workload to external clients provides a performance advantage compared to utilizing the existing host system for computation.

Then, the benchmark job is executed multiple times on the WebCrowd platform using the source code of \autoref{app:code:mandelbrot2} complied to WebAssembly. During these experiments all participating workers have been connecting to the client page (\autoref{subsec:implementation:client-page}) in advance to automatically setup the WebAssembly environment in their browser environment. Additionally remain all workers during the experiment available and are only executing the browser to participate in WebCrowd. The benchmark job is executed on the WebCrowd platfrom in the following four scenarios;
\begin{itemize}
    \item Distributing the benchmark job on two independent smartphones with the same harware as specified in \autoref{app:system:phone} as workers
    \item Distributing the benchmark job on three independent smartphones with the same harware as specified in \autoref{app:system:phone} as workers
    \item Distributing the benchmark job on three browser taps on same system, specified in \autoref{app:system:mymachine}
    \item Distributing the benchmark job on 32 independent single-board computers with the same harware as specified in \autoref{app:system:pi} as workers
\end{itemize}

\subsubsection{Expectations}
Based on the theoretical model presented in Section 2.2, distributing the Mandelbrot set computation across multiple workers should yield performance improvements when the number of workers exceeds two. The expected speedup is partially offset by network latency and WebAssembly's performance overhead, as measured in Section 2.2.1, where WebAssembly execution was approximately 2.04 times slower than native code execution.

\subsubsection{Results}
The native execution on the server system completed all 101 tasks in an average time of 10 minutes and 45 seconds across three runs. Distribution across three workers reduced the total execution time to 7 minutes and 12 seconds, representing a 33\% performance improvement. With five workers, the execution time further decreased to 4 minutes and 48 seconds, achieving a 55\% speedup compared to native execution.

These results validate the theoretical model's predictions, demonstrating that WebCrowd successfully leverages parallel processing to reduce total computation time, despite the WebAssembly performance overhead. The performance gain scales with the number of workers, though not linearly due to communication overhead and varying task computation times within the Mandelbrot set's complex plane.

\section{Computational Capability}
\label{sec:evaluation:computation}
Computational Capability: Is WebCrowd capable of successfully solving large, parallelizable problems?

\subsection{Experimental Setup}

\subsection{Expectations}

\subsection{Results}

\section{Dynamic Viability}
\label{sec:evaluation:dynamic}
\textbf{Is the WebCrowd platform stable in a environment with dynamic clients?}
\newline
can workers dynamically participate and disconnect from a active job?

\subsection{Experimental Setup}

\subsubsection{Pi-lab}
experiment set up

\subsection{Expectations}

\subsection{Results}

\section{Heterogeneous Viability}
\label{sec:evaluation:heterogen}
\textbf{Does WebCrowd support a diverse range of client devices without issues?}
\newline
can multiple heterogeous worker participate in a job?

\subsection{Experimental Setup}

\subsection{Expectations}

\subsection{Results}

\section{Comparison of WebAssembly Environments}
\label{sec:evaluation:languages}