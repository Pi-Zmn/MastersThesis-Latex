\chapter{Background and related Work}
\label{ch:background}
This chapter covers the theoretical background of this work. At first \autoref{sec:background:volunteercomputing} introduces the concept of volunteer computing. Then \autoref{sec:background:webassembly} provides technical information about WebAssembly, which is the key web technologie utilized in WebArgo. And finally \autoref{sec:background:related_work} presents related works to describes the state of research and development in the field of volunteer computing and edge computing with WebAssembly.

\section{Distributed Computing \& Volunteer Computing}
\label{sec:background:volunteercomputing}
Distributed computing describes a architecture where many autonomous computing elements collaborate as a unified system to solve complex computational problems through coordinated sharing of resources. A distributed computing system typically consists of multiple independent nodes, each possessing local memory and computational resources, interconnected through a network infrastructure that facilitates communication and synchronization between processes. This concept is particularly suitable to compute parallelizable workloads.

Volunteer computing is a sub category of distributed computing that focuses on public support. Participants share their computer processing power to support a collective interest. The idea of volunteer computing already emerged in 1996 \cite{relatedwork:boinc1} and \citeauthor{background:vcname} later characterized the term \emph{volunteer computing} \cite{background:vcname}. The first large-scale scientific projects SETI@home and Folding@Home where launched in 1999 \cite{relatedwork:boinc1,relatedwork:seti}. These pioneering projects have demonstrated the success and popularity of volunteer computing.

Since the total amount of consumer devices like laptops, smartphones, tablets and desktop computers continues to increase \cite{background:amountdeviceses,relatedwork:boinc1} there is a large pool of devices that can be used for volunteer computing. A study in 2014 has estimated that about 2 Billion computers are actively in use worldwide \cite{intro:computersAmount}. However, many of these resources often remain idle or underutilized during regular operation \cite{relatedwork:mobilecloud, relatedwork:wasmedgecomputing}. While these untapped resources may seem insignificant individually, they collectively represent substantial potential due to their total number. These available resources located in office buildings, universities or public spaces can be utilized to participate in volunteer computing projects and thereby share their unused computational capabilities.

Furthermore, a study of 2011 revealed significant potential acceptance of volunteer computing in society, with 37\% of respondents indicating they were either \emph{"somewhat likely"} or \emph{"very likely"} to participate in volunteer computing projects \cite{intro:volunteerStudy}. However, the study also identified privacy and security concerns as primary factors that negatively influence willingness to volunteer in such projects \cite{intro:volunteerStudy}.

\section{WebAssembly}
\label{sec:background:webassembly}
WebAssembly is a low-level binary instruction format designed to serve as a compilation target for high-level programming languages \cite{methodology:wasm, methodology:wasmW3C, methodology:wasm2}, that promises near-native performance execution in web browsers \cite{methodology:wasm, methodology:wasmW3C, relatedwork:wasmedgecomputing}. It employs a stack-based virtual machine architecture, operates in a memory-safe sandboxed environment, and interacts with JavaScript of the browser environment through a defined \ac{API} \cite{methodology:wasm, methodology:wasmW3C, methodology:wasm2, methodology:wasmdocu}. Utilizing WebAssembly is particularly effective for computationally intensive tasks in the browser \cite{methodology:wasm2, methodology:wasmW3C} like image processing, game engines, or cryptographic operations.

Furthermore, WebAssembly is bound by the browsers inherent security model \cite{methodology:wasmW3C, methodology:wasm2, methodology:wasmdocu}. This is preventing any application to read and write Ô¨Åles or to access device hardware such as cameras or microphones without explicit user permissions. 
\\~\\
Leveraging WebAssembly offers several advantages for distributed or edge computing \cite{relatedwork:wasmedgecomputing}. It provides near-native performance, while maintaining platform independence \cite{methodology:wasm, methodology:wasmW3C, relatedwork:wasmedgecomputing}. These flexible features of platform independence and the support of multiple high-level programming languages targeting the WebAssembly binary format \cite{methodology:wasmW3C} was the motivation behind the development of WebArgo. These features make WebAssembly a fantastic choice and enabeling technology for the implementation of a platform like WebArgo. The web environment additionally provides the previously mentioned inherent security model. Therefore, WebArgo potentially provides a higher security for worker nodes than other volunteer computing applications, which require the installation of third-party software directly to the machine. This directly addresses the privacy and security concerns revealed by the study of \citeauthor{intro:volunteerStudy} \cite{intro:volunteerStudy} to increase the potential pool of participating workers.

\section{Related Work}
\label{sec:background:related_work}
This section presents relevant research and implementations in the fields of volunteer computing and edge computing with WebAssembly or mobile devices. Each project discussed has contributed unique insights and approaches to addressing the challenges inherent in distributed computing systems. The first subsection of this section focuses on historic volunteer computing projects or implementations and also introduces \acs{BOINC}, which is an actively maintained volunteer computing platform. The summaries of these various related works establish a foundation for WebArgo's motivation and validate WebArgo's approach of distributed computation in the modern web ecosystem.

\subsection{Volunteer Computing}
The idea of volunteer computing has been established for a long time. The earliest concept of volunteer computing started already in 1996 \cite{relatedwork:boinc1}. This section introduces relevant projects in this field, which have been used in the past or are currently in active use and maintained.

\subsubsection{SETI@home}
\label{subsec:background:related_work:seti}
\ac{SETI} at home (\ac{SETI}@home) represents a pioneering project in the field of distributed and volunteer computing. It utilizes the resources of volunteer computers worldwide to analyze radio telescope data in search of extraterrestrial intelligence. Data from the Arecibo radio telescope is divided into smaller chunks which are distributed to volunteer computers for processing. The system is designed to identify several types of patterns in radio signals that could indicate an artificial origin. These anormalies are spikes, Gaussians, pulsed signals, and triplets in the collected radio data. \cite{relatedwork:seti}

In the year 2002, \ac{SETI}@home had engaged over 3.8 million participants across 226 countries, achieving an average computing power of 27.36 Tera\acs{FLOPS} (\ac{FLOPS}) \cite{relatedwork:seti}.

\ac{SETI}@home demonstrated the viability of large-scale public-resource computing and identified key factors that make tasks suitable for this approach:
\begin{itemize}
  \item High computing-to-data ratio
  \item Independent parallelism
  \item Error tolerance
  \item Ability to attract public interest
\end{itemize}
The success of \ac{SETI}@home not only advanced scientific research but also increased public awareness and involvement in scientific participation. This project laid the groundwork for present volunteer computing projects and frameworks. \cite{relatedwork:seti}
\\~\\
The \ac{SETI}@home project is to this day maintained and actively supported by volunteer computing participants. Currently \ac{SETI}@home is managed through the \acs{BOINC} platform \cite{relatedwork:boinc1}, introduced in \autoref{subsec:background:related_work:boinc}.

\subsubsection{XtremWeb}
\label{subsec:background:related_work:xtremweb}
XtremWeb is an experimental global computing platform designed to harness idle computing resources connected to the internet for high-throughput computing. The system aims to provide a platform for experimenting with global computing capabilities and addressing issues such as scalability, heterogeneity, availability and fault tolerance in massively distributed computing environments. \cite{relatedwork:xtremweb}

The architecture of XtremWeb consists of three main components:
\begin{itemize}
  \item \textbf{Workers}: These are volunteer \acs{PC}s that execute tasks. They are implemented primarily in Java for portability, with native code used for \ac{OS}-specific functions. Workers monitor resource availability based on user-defined policies and support multiple platforms, including Linux and Windows.
  \item \textbf{Servers}: These manage tasks and applications. The server design is modular, with components for application pool, job pool, accounting, and scheduling. Servers can be clustered for increased throughput and support specialization for tasks such as dedicated result collection.
  \item \textbf{Clients}: These submit tasks to the system.
\end{itemize}
XtremWeb employs a two-part scheduling system:
\begin{enumerate}
  \item \textbf{Dispatcher}: Selects tasks from the pool based on application priorities.
  \item \textbf{Scheduler}: Assigns tasks to workers.
\end{enumerate}
The default scheduling policy of XtremWeb is \ac{FIFO} and XtremWeb implements a timeout to enable rescheduling of aborted tasks \cite{relatedwork:xtremweb}.

Furthermore XtremWeb implements a worker-initiated communication protocol, which facilitates easier deployment through firewalls. This protocol consists of the four main requests: \emph{hostRegister}, \emph{workRequest}, \emph{workAlive}, and \emph{workResult}. \cite{relatedwork:xtremweb}

XtremWeb has been successfully applied to various projects in the past. These include the Pierre Auger Observatory for studying high-energy cosmic rays. In this application, XtremWeb was used to run the \ac{AIRES} program by partitioning large simulations into smaller subtasks. \cite{relatedwork:xtremweb}
\\~\\
In summary, XtremWeb provides a flexible and robust platform. Its approach and concept has some similarities to WebArgo, but the original XtremWeb project seems not to be maintained since its development in the early 2000s.

\subsubsection{EGEE}
\label{subsec:background:related_work:egee}
Initiated on April 1, 2004, \ac{EGEE} was a project planned to be maintained over four years, involving 71 partners from Europe, Russia, and the United States \cite{relatedwork:egee}. The project's primary objectives were to establish a seamless European grid infrastructure for scientific research, provide production-level grid services, and re-engineer grid middleware for enhanced robustness and scalability \cite{relatedwork:egee}.

The infrastructure of \ac{EGEE} was built upon the EU Research Network G√âANT \cite{background:geant}, leveraging expertise from previous initiatives such as EU DataGrid, UK e-Science, INFN Grid, Nordugrid, and US Trillium. The project aimed to expand its computational capabilities from an initial 3000 \acs{CPU}s at 10 sites after the first month to 10000 \acs{CPU}s across 50 sites by the end of the second active year \cite{relatedwork:egee}.

\ac{EGEE} focused on two primary pilot applications: 
\begin{itemize}
  \item The \emph{\ac{LCG}} for high-energy physics data analysis.
  \item \emph{Biomedical Grids} addressing challenges in genomic database mining and medical database indexing
\end{itemize}
The project focused on re-engineering existing middleware to address issues from first-generation implementations and ensure adherence to \ac{OGSA} standards. This approach aimed to enhance the reliability and scalability of the grid infrastructure \cite{relatedwork:egee}.
\\~\\
In conclusion, the \ac{EGEE} project represented a significant effort to transform grid computing from a research concept into a practical infrastructure to support "e-science" across Europe.

\subsubsection{BOINC}
\label{subsec:background:related_work:boinc}
\ac{BOINC} is an open-source middleware system for volunteer computing that enables scientists to create and operate volunteer computing projects while allowing volunteers to participate in these projects \cite{relatedwork:boinc1}. It addresses key challenges inherent in distributed computing systems, such as dealing with untrusted, unreliable, and heterogeneous computing resources, validating results from potentially malicious hosts, and supporting diverse applications and computing environments.

The \ac{BOINC} architectural design implements two components that interact through \acs{HTTP}-based \ac{RPC} interfaces \cite{relatedwork:boinc1}. These components are the \emph{\ac{BOINC} Server} and multiple \emph{\ac{BOIC} Client} components.

At its core, the \emph{\ac{BOINC} Server} infrastructure consists of: a relational database (typically MySQL or MariaDB) that maintains tables for volunteer accounts, hosts, applications, versions, jobs, and job instances; a scheduler implemented as a \ac{CGI} or \ac{FCGI} program that handles \acs{RPC}s from \emph{\ac{BOIC} Clients}; and a shared-memory job cache that typically holds thousands of jobs for efficient dispatch \cite{relatedwork:boinc1}. The scheduler's design is particularly noteworthy - rather than directly querying the database for each job dispatch, it utilizes a shared-memory segment containing a cache of available job instances, which is continuously replenished by a feeder process \cite{relatedwork:boinc1}. This architectural decision allows a single \emph{\ac{BOINC} Server} to efficiently dispatch hundreds of jobs per second. The \emph{\ac{BOINC} Server} additionally employs multiple components that work together to manage the system:
\begin{itemize}
    \item \textbf{Validator}: compares the output files of results and determines the validity of a result
    \item \textbf{Assimilator}: processes completed and validated jobs and handles their results according to project-specific requirements
    \item \textbf{Transitioner}: manages job state transitions in a finite-state model
    \item \textbf{File deleter}: removes input and output files of completed jobs
    \item \textbf{Database purger}: maintains database efficiency by removing records of completed jobs
\end{itemize}

The software of the \emph{\ac{BOIC} Clients} component consists of three main programs that communicate via \acs{RPC}s over \acs{TCP} connections: the core client that manages job execution and file transfers, a \acs{GUI} (the so called \emph{\ac{BOINC} Manager}) that enables user control and monitoring, and an optional screensaver for displaying application graphics \cite{relatedwork:boinc1}. The \emph{\ac{BOIC} Clients} implements scheduling policies to manage multiple projects and handle various resource types including \acs{CPU}s and \acs{GPU}s \cite{relatedwork:boinc1}. It also maintains job queues and implements checkpoint/restart capabilities to handle interrupted computations \cite{relatedwork:boinc1}. 

Furthermore \ac{BOINC} implements a result validation system to handle potential incorrect results caused by hardware errors or malicious behavior of volunteering clients \cite{relatedwork:boinc1}. At its core, the system employs replication-based validation where each job is processed on multiple unrelated computers. Initially, when a quorum of successful instances is achieved (typically two or more), their outputs are compared. If the outputs agree, one instance is designated as the canonical result and considered correct \cite{relatedwork:boinc1}. However, due to variations in floating-point hardware and math libraries across different platforms, bitwise-identical results cannot always be expected. Therefore, \ac{BOINC} allows projects to supply application-specific validator functions that determine result equivalence based on specified tolerances \cite{relatedwork:boinc1}. For applications with inherent numerical instability, such as physical simulations, \ac{BOINC} provides a mechanism called homogeneous redundancy, which groups computers into equivalence classes based on their hardware and software configurations, ensuring that subsequent instances of a job are only dispatched to computers within the same equivalence class \cite{relatedwork:boinc1}. To optimize computational efficiency, \ac{BOINC} employs adaptive replication of jobs. This adaptive replication reduces the replication factor of a job close to one by identifying clients that consistently return correct results and therefore are clients \cite{relatedwork:boinc1}. The system maintains this "reputation" at the granularity of host-application version pairs, as some computers may be reliable for \acs{CPU} jobs but unreliable for \acs{GPU} jobs. This adaptive approach achieves a low error rate while minimizing the computational overhead typically associated with result validation.

The \ac{BOINC} platform has demonstrated significant success in supporting scientific computing, with approximately 700,000 devices actively participating across various projects \cite{relatedwork:boinc1}. These devices collectively provide an average throughput of 93 Peta\ac{FLOPS}, utilizing about 4 million \acs{CPU} cores and 560,000 \acs{GPU}s \cite{relatedwork:boinc1}. \ac{BOINC}'s versatility is evident in its support for various scientific applications, including standard programs like Autodock, Gromacs, Rosetta, LAMMPS, and BLAST \cite{relatedwork:boinc1}. The platform can handle applications that use GPUs (through CUDA and OpenCL), multiple CPUs (via OpenMP or OpenCL) and applications running in virtual machines or Docker containers \cite{relatedwork:boinc1}.

Furthermore, \ac{BOINC} implements a robust security model where applications run under an unprivileged user account, provides checkpointing capabilities to handle interrupted computations, and includes features for monitoring and controlling resource usage \cite{relatedwork:boinc1}. It is estimated that \ac{BOINC} projects can achieve approximately 2 Peta\ac{FLOPS} of computing power at an annual cost of around \$100K, what is assumed to be significantly less expensive than equivalent commercial cloud computing solutions  \cite{relatedwork:boinc1}.
\\~\\
While both \ac{BOINC} and WebArgo are volunteer computing platforms, their architectural approaches and implementation strategies differ significantly. \ac{BOINC} requires volunteers to install a dedicated client application that handles job execution and project management \cite{relatedwork:boinc1}, whereas WebArgo takes a simplified web approach that requires for client devices only access to a web browser, which supports WebAssembly, WebSockets, and WebWorker. This architectural difference significantly impacts the barrier of entry for volunteers - WebArgo's browser-based approach eliminates installation requirements and potential security concerns associated with executing third-party applications on a personal device. However, \ac{BOINC}'s architecture includes multiple advanced features like adaptive result validation and homogeneous redundancy or the execution of multiple jobs simultaneously, whereas WebArgo's implementation is currently limited in these functionalities. Both systems address the challenge of heterogeneous computing environments, but through different means - \ac{BOINC} through its complex platform-specific application versions and plan classes, and WebArgo through WebAssembly's inherent platform independence.

Furthermore, while \ac{BOINC} requires project-specific servers, a significant setup overhead and organizations to pay a significant amount to distribute their projects, WebArgo focuses on simplicity and accessible for these organizations. WebArgo aims to minimize administrative overhead that every organization can host their own volunteer computing platform to distribute their research projects.

\subsection{Edge Computing}
Edge computing represents a paradigm shift in distributed computing architectures by moving computational resources closer to where data is generated and consumed. This approach is expected to reduces latency, since the data and computing device are physically closer. Therfore, edge computing is associate to enable real-time processing capabilities for systems. The following subsections examine research leveraging WebAssembly or mobile devices for edge computing scenarios.

\subsubsection{WebAssembly for Edge Computing}
\label{subsec:background:related_work:wasmedgecomputing}
The work from \citeauthor{relatedwork:wasmedgecomputing} \cite{relatedwork:wasmedgecomputing} investigates WebAssembly as a promising solution for edge computing challenges, particularly addressing the crucial requirements of portability and migratability for such applications. In this work WebAssembly is compared to virtual machines, containers, Java, JavaScript and a native environment as edge computing environment. This work builds upon previous research in edge computing virtualization while introducing novel perspectives on using WebAssembly as an enabling technology for portable and migratable edge applications. Their comprehensive analysis demonstrates WebAssembly's capabilities in achieving near-native performance while maintaining platform independence, which proposes a significant advancement compared to traditional virtualization approaches. In their work the authors present a systematic evaluation of different WebAssembly execution environments, including browser-based implementations and standalone runtimes, focusing on the performance to portability trade-off.

Their findings highlight WebAssembly's potential as a lightweight alternative to conventional virtual machines or container based approaches in edge computing scenarios \cite{relatedwork:wasmedgecomputing}. However, they identified that WebAssembly is currently lacking migratability features, compared to virtual machines and containers, to make it a complete solution for edge computing environments \cite{relatedwork:wasmedgecomputing}. 

Depending on the migration scenario they provide ideas for further actions or development to utilize WebAssembly in edge computing. These four drafts of migration methods as proposed in their work are:
\begin{enumerate}
  \item \textbf{Cold Migration}: The simplest approach where the WebAssembly binary is completely restarted on another new device and only persistent data like disk writes is migrated to the new device. This method can be enhanced to "semi-live" migration by keeping the JavaScript runtime state while restarting only the WebAssembly modules. This method is suited for small, standalone workloads where occasional restarts are acceptable. \cite{relatedwork:wasmedgecomputing}
  \item \textbf{Interpreter-Based}: Runs WebAssembly code through an interpreter instead of compiling it, making it easier to capture and transfer program state since the interpreter has full visibility into execution. While this enables straightforward live migration, it comes with significant performance overhead, running much slower than a compiled WebAssembly binary. \cite{relatedwork:wasmedgecomputing} 
  \item \textbf{WebAssembly Instrumentation}: Modifies the WebAssembly code to track its own state during runtime. When migration is needed, the instrumented code dumps its state, which is then used to resume execution on the target device. While this maintains better performance than interpretation, it faces challenges with low-level state management and increases code size with each migration. \cite{relatedwork:wasmedgecomputing}
  \item \textbf{Binary Instrumentation}: Works at the native binary level, where the WebAssembly compiler produces architecture-specific binaries with built-in migration capabilities. The compiler ensures equivalent migration points across different architectures. This requires careful coordination to maintain uniform data and code layout across different platforms but can potentially offer the best performance among migration options. \cite{relatedwork:wasmedgecomputing}
\end{enumerate}
~\\
The investigattion of this work validates, that the WebAssembly technology can be leveraged to develop a volunteer computing platform like WebArgo. Additionally, WebArgo has implemented a similar approach to the proposed "Cold Migration" method to distribute and reschedule workloads among volunteer participants.

\subsubsection{Dynamic Mobile Device Clusters in Edge Femtoclouds}
The work from \citeauthor{relatedwork:mobilecloud} \cite{relatedwork:mobilecloud} presents an enhanced architecture for Femtoclouds that enables mobile device clusters to provide edge computing services while using the cloud for control and management. As described by \citeauthor{relatedwork:mobilecloud}, a Femtocloud is a cluster of co-located mobile devices in places such as public transit, classrooms, or coffee shops that work together to provide edge computing services, similar to a traditional cloud but on a smaller scale \cite{relatedwork:mobilecloud}. The key innovation is that these everyday mobile devices can collectively serve as a computational resource when properly managed, offering advantages like lower latency and reduced network congestion compared to traditional cloud services. \citeauthor{relatedwork:mobilecloud} explored the concept of these edge femtoclouds, which have demonstrated a significant performance improvements through collaborative mobile edge computing in their work \cite{relatedwork:mobilecloud}. In their experiment they compared the execution time of a matric multiplication job on a regular cloud environment compared to their Femtocloud, consiting of 6 mobile devices with the Android \ac{OS}. The tested prototype was implemented as an Android application. Their experimental results show that a their edge Femtocloud can reduce the job completion time of their matrix benchmark by up to 26\% compared to traditional cloud computing approaches, with their implemented checkpointing mechanism further improving efficiency by an additional 31\% \cite{relatedwork:mobilecloud}. 

The authors developed a Femtocloud architecture to addres the challenges of device churn and system stability. This Femtocloud architecture consists of the following three main components: 
\begin{itemize}
  \item \textbf{Femtocloud Controller}: A cloud-based controller that handles registration, job admission, and resource management, serving as a stable interface between job originators and helper devices
  \item \textbf{Femtocloud Helpers}: Mobile devices, which run the client application that executes computation tasks and shares their resources based on user-defined policies
  \item \textbf{Job Managers}: A instance, which is spawned for each accepted job to handle task assignments, monitor progress, and manage task checkpointing, which can run either in the cloud or be migrated to Femtocloud Helpers or job originators for better efficiency
\end{itemize}
~\\
The system uses a hybrid approach where the cloud handles control and management while the actual computation happens at the edge through the mobile device clusters, with all components working together to handle job distribution, resource allocation, and fault tolerance in a dynamic environment where devices can join and leave the system. This system recieves an amount of external jobs, defined by the so called "job arrival rate". These recieved jobs are than distributed among available Femtocloud Helpers.

Their prototype demonstrated robust performance across varying job arrival rates \cite{relatedwork:mobilecloud}. While highlighting the benefits of reduced latency and network congestion, their findings also identified key challenges including the need for sufficient device participation and robust security measures \cite{relatedwork:mobilecloud}. The work presents Edge Femtoclouds as a cost-effective alternative to dedicated edge servers. 

Furtheremore, the authors mention that implementing an effective user incentive mechanism is one of the open issues that needs to be addressed before these Femtoclouds are fully deployable. The paper includes a pilot study on user incentives that surveyed about 50 students to understand what would motivate users to share their resources. They found two main motivators:
\begin{itemize}
  \item Financial compensation (when supporting for-profit companies)
  \item Meaningful causes (76\% would participate for scientific purposes, 83\% for emergency cases like finding a lost child)
\end{itemize}
~\\
\citeauthor{relatedwork:mobilecloud} stated that the support of heterogen devices is a important feature for such a application, due to the heterogeneous characteristic of the mobile device environment \cite{relatedwork:mobilecloud}. However, the implemented prototype that was built in their work is only supporting android devices as a proof of concept.
\\~\\
The work of \citeauthor{relatedwork:mobilecloud} has a different implementation approach to WebArgo, but has a similar motivation. The implementation of WebArgo additionally takes the development of custom jobs, simple usability for participants and supoport for devices with heterogenous operating systems into consideration compared to Femtoclouds. However, their experimental results validate, that ubiquitous mobile devices can be utilized to perform distributed computation and thereby achieve a performance improvement compared to traditional cloud methods.