\chapter{Background and related Work}
\label{ch:background}
TODO: Chapter 1 blabla

This chapter covers the theoretical foundation of this work in the first section. A theoretical example case is used to ilustrate the expected performance gains through using the platform. The following section presents other related works to describes the state of research and development in the field of volunteer and crowd computing.

TODO: Present state of VC or CC -> what makes my work different?

\section{TODO}
TODO: what is crowd computing or volunteer computing -> what makes my work different? 

\section{Theory}
\label{sec:background:theory}
This following section displays the mathematical and theoretical background of a distributed computing network. The objective is to identify the constraints that must be satisfied to ensure that the use of a distributed computing network offers advantages over regular local computing on a single device.

The primary constraint is that the program code, which is distributed across multiple devices, must be capable of parallel execution. This allows all participating devices to  each compute a portion of the overall workload simultaneously. In this context, a parallelizable problem, as defined in this thesis, is characterized by the following properties:
\begin{itemize}
    \item The problem can be divided into multiple tasks.
    \item Each task executes the same program code.
    \item Each task can receive specific input parameters to process a distinct part of the problem.
    \item Each task operates independently, with no need for communication between tasks.
\end{itemize}
To calculate the execution time of a parallelizable problem, the problem is divided into \emph{T} equally sized tasks. It is assumed that, on average, each task requires a computing time of $t_{C}$ when executed on a single device. Consequently, the total computation time for a parallelizable problem on a single device is expressed as:
\begin{equation}
T \cdot t_{C}
\label{equ:single}
\end{equation}
When the tasks \emph{T} are executed in parallel on \emph{N} nodes, the total computation time is reduced proportionally by distributing the workload. This is implemented through the fraction of \emph{T} and \emph{N}. Since \emph{T}, \emph{N} and the result of their division need to be natural numbers ($\mathbb{N}$), the gaussian ceiling function is applied. The ceiling function values any remainder as a additional value when dividing tasks among nodes and therefore always returns a value in range of $\mathbb{N}$. Hence, the fraction is given by: 
\begin{equation}
\bigg\lceil\frac{T}{N}\bigg\rceil
\label{equ:frac}
\end{equation}
When a problem is distributed across \emph{N} nodes over a wireless connection, the network latency must be taken into account in the computation. The latency, denoted as $t_{L}$, consists of the time required to send input arguments to a single device and the time needed to receive the result from a single task. It is assumed that the program code necessary to execute a task is loaded on all devices in advance, and therefore, the initial overhead of this process is excluded from the calculation. Additionally, it is assumed that the computation time, $t_{C}$, is equally long across all \emph{N} nodes. Thus, the total execution time for a parallelizable problem distributed across \emph{N} nodes can be expressed as: 
\begin{equation}
\bigg\lceil\frac{T}{N}\bigg\rceil \cdot (t_{C} + t_{L})
\label{equ:multiple}
\end{equation}
The objective is to reduce the total computation time of a problem by distributing the workload across multiple nodes in parallel. This means that the execution time on a single device, as described by \eqref{equ:single}, must be longer than the execution time on multiple devices, as described by \eqref{equ:multiple}. This leads to following inequality expression:
\begin{equation}
T \cdot t_{C} > \bigg\lceil\frac{T}{N}\bigg\rceil \cdot (t_{C} + t_{L})
\label{equ:compare}
\end{equation}
In order to transform this inequality, the term needs to be simplified by substituting the gaussian bracket. The fraction \eqref{equ:frac} can be replaced by an overestimate, without altering the logical validity of the inequality:
\begin{equation}
\bigg\lceil\frac{T}{N}\bigg\rceil < \frac{T}{N} + 1
\label{equ:frac2}
\end{equation}
Substituting this overestimate into the inequality allows for the following transformation:
\begin{alignat}{4}
 T \cdot t_{C} &> \bigg(\frac{T}{N} + 1\bigg) \cdot (t_{C} + t_{L}) \nonumber \\
 T \cdot t_{C} &> \bigg(\frac{T + N}{N}\bigg) \cdot (t_{C} + t_{L}) \nonumber \\
 \frac{T \cdot N \cdot t_{C}}{T + N} &> t_{C} + t_{L} \nonumber \\
 \frac{T \cdot N \cdot t_{C}}{T + N} - t_{C} &> t_{L} \nonumber \\
 t_{C} \cdot \bigg(\frac{T \cdot N}{T + N} - 1\bigg) &> t_{L}
 \label{equ:transformation}
\end{alignat}
The last expression through the transformation in \eqref{equ:transformation} results in the constrain, that distributing a parallelizable problem across multiple nodes is only beneficial if the latency $t_{L}$ is shorter than the computation time of a single task multiplied by the factor $(\frac{T \cdot N}{T + N} - 1)$.

Additionally the expression \eqref{equ:frac} can be used to determine the optimal amount of nodes \emph{N} for a problem divided in \emph{T} equally sized tasks. This expression reaches its minimum when \emph{N} equals \emph{T}. This outcome is intuitive, as in this ideal scenario each Node is rersponsible for a single task and therefore all tasks \emph{T} are executed simultaneously. Furthermore can be concluded from \eqref{equ:frac} that the amount of nodes should ideally be a divisor of \emph{T} to ensure a efficient utilization of all nodes. However, in practice, it should be considered to include additional backup devices in the network to account for potential failures or disruptions.

\subsection{Theoretical Case}
To illustrate the performance gain achieved by a distributed computing network, both expressions \eqref{equ:single} and \eqref{equ:multiple} are plotted in figure \ref{fig:background:theoryplot}. To calculate these graphs, an Internet latency of 32 ms \cite{backend:latency} was assumed. This value is the \ac{IQI} estimated round trip time within Europe \cite{backend:latency}. The average computation time $t_{C}$ for a single task was assumed to be 34.97 s, based on the measured average run time of the source code in \ref{app:code:mandelbrot1} on the system specified in \ref{app:system:mymachine}. This measurment was repeated with a WebAssembly binary, generated with the source code in \ref{app:code:mandelbrot2}. The average execution time of the WebAssembly binary in a browser environment on the system specified in \ref{app:system:mymachine} was measured at 71.17 seconds. 

This unexpected difference in computation duration is very likely due to the use of 64-bit and 128-bit values in the source code of \ref{app:code:mandelbrot1} and \ref{app:code:mandelbrot2}. Since the WebAssembly environment is constrained to 32-bit operations (TODO: ADD WASM SOURCE), the conversion process from multiple 32-bit values to 64-bit or 128-bit values could introduce computational overhead, extending the execution time of the WebAssembly binary. (TODO: Use explanation of "Not so fast!")

An initial offset time is additionally considered to establish the distributed computing network. In this process, each node must download all necessary files in advance to be able to execute the workload. Since this setup occurs in parallel across all nodes, the offset matches the time required for the setup of a single node. This offset value is calculated using the \ac{IQI} estimated download speed in Europe of 29 Mbps \cite{backend:latency}. The size of the compiled WebAssembly binary file from the source code in \ref{app:code:mandelbrot2} is 21.6 Mb (2.7 MB), and the coresponding \emph{wasm\_exec.js} JavaScript file to setup the WebAssembly environment is 0.13 Mb (16.7 KB), resulting in a total file size of 21.76 Mb. The time required to download both files to a single node can be estimated using the following expression:
\begin{equation}
 Download-Time = Latency + \frac{Filesize}{Bandwith} 
 \label{equ:offset}
\end{equation}
Since the bandwidth is given in megabits per second, while the latency is provided in milliseconds, the latency value needs to be converted to seconds. Using the calculation in \ref{equ:offset}, the additional offset that is required to set up the platform, in this theoretical case, is estimated to be at least 0.78 s.

\begin{figure}[bth]
  \myfloatalign
  \subfloat[Variable amount of nodes N]{
     \label{fig:background:theoryA}
     \includegraphics[width=.75\linewidth]{gfx/figures/Theory_A.png}
   } \\
   \subfloat[Variable amount of tasks T]{
     \label{fig:background:theoryB}
     \includegraphics[width=.75\linewidth]{gfx/figures/Theory_B.png}
   }
   \caption{Visualizing the Total Computation Time for an Estimated Example Case}
   \label{fig:background:theoryplot}
\end{figure}

The graph in Figure \ref{fig:background:theoryA} represents the total runtime, on the y-axis in seconds, as a function of the number of nodes in the network, on the x-axis. The red dotted line displays the performance of a single machine, while the blue line illustrates the performance of the distributed computing network. The total number of all tasks to be executed is set to 20. This plot reveals that the distributed network achieves a faster total computation time than a single device, if three or more nodes are availabile in the network. The total runtime is already reduced by 30\% when the workload is distributed to three nodes.

The second graph in Figure \ref{fig:background:theoryB} displays the total runtime on the y-axis in seconds as a function of the number of tasks on the x-axis. The red dotted line represents again a single device and the blue line the distributed computing network. The number of nodes N is set to three. This plot demonstrates that the distributed network achieves faster total computation time than a single device when the workload consists of more than two tasks. The performance gain grows exponentially as the number of tasks increases. Additionally, the total computation time of the network only increases when the number of tasks T and the number of nodes N have no common divisor. However, it is crucial to note that the network will have a longer computation time in every scenario with only one task, as a single task cannot be executed in parallel.

Both plots show, that the total computation time is already almost even in the distributed network if the network consists of two nodes and faster if there are three or more nodes. This holds true even if the computation time $t_{C}$ of a single task is twice as fast on a native system then in the distributed environment. This proves that the approach to distribute the workload is a performance gain in this example, if multiple nodes are availabile. But it needs to be considdered, that if there is only one or two node in the network this approach will be slower due to the offset time, the additional network latency and the higher computation time for each individual task.


\section{Related Work}
\label{sec:background:related_work}
The idea of distributed computing or volunteer computing has been established for a long time. The earliest concept of volunteer computing started already back in 1996 \cite{relatedwork:boinc1}. This section introduces relevant projects in this field, which have been used in the past or are currently actively used and maintained.   
\subsection{BOINC}
\label{subsec:background:related_work:boinc}
\ac{BOINC} is an open-source middleware system for volunteer computing, which utilizes consumer devices for scientific computing. It addresses key challenges in distributed computing, such as dealing with untrusted, unreliable, and heterogeneous computing resources, validating results from potentially malicious hosts, and supporting diverse applications and computing environments. \cite{relatedwork:boinc1}

The BOINC architecture consists of server components, including a database and job scheduler, and client software comprising a core client, GUI, and screensaver. Communication between clients and servers is facilitated through HTTP-based RPCs. BOINC employs replication-based validation and adaptive replication to ensure result integrity while minimizing overhead. The system also implements sophisticated scheduling policies both on the client and server sides to optimize resource allocation and job dispatch. \cite{relatedwork:boinc1}

BOINC has been successful in supporting numerous scientific projects and providing substantial computing power. \cite{relatedwork:boinc1}

\subsection{SETI@home}
\label{subsec:background:related_work:seti}
\ac{SETI} at home (SETI@home) represents a pioneering project in the field of distributed and volunteer computing. It utilises the resources of volunteer computers worldwide to analyze radio telescope data in search of extraterrestrial intelligence. Data from the Arecibo radio telescope is divided into smaller chunks which are distributed to volunteer computers for processing. The system is designed to identify several types of patterns in radio signals that could indicate an artificial origin. These anormalies are spikes, Gaussians, pulsed signals, and triplets in the collected radio data. \cite{relatedwork:seti}

Back in 2002, \ac{SETI}@home had engaged over 3.8 million participants across 226 countries, achieving an average computing power of 27.36 teraFLOPS. \cite{relatedwork:seti}

\ac{SETI}@home demonstrated the viability of large-scale public-resource computing and identified key factors that make tasks suitable for this approach:
\begin{itemize}
  \item High computing-to-data ratio
  \item Independent parallelism
  \item Error tolerance
  \item Ability to attract public interest
\end{itemize}

The success of \ac{SETI}@home not only advanced scientific research but also increased public awareness and involvement in scientific participation. This project laid therefore the groundwork for future volunteer computing projects and frameworks. \cite{relatedwork:seti}

\subsection{XtremWeb}
\label{subsec:background:related_work:xtremweb}
XtremWeb is an experimental Global Computing platform designed to harness idle computing resources connected to the Internet for high-throughput computing. The system aims to provide a platform for experimenting with Global Computing capabilities and addressing issues such as scalability, heterogeneity, availability, fault tolerance, security, and usability in massively distributed computing environments. \cite{relatedwork:xtremweb}

The architecture of XtremWeb consists of three main components:
\begin{enumerate}
  \item Workers: These are volunteer PCs that execute tasks. They are implemented primarily in Java for portability, with native code used for OS-specific functions. Workers monitor resource availability based on user-defined policies and support multiple platforms, including Linux and Windows.
  \item Servers: These manage tasks and applications. The server design is modular, with components for application pool, job pool, accounting, and scheduling. Servers can be clustered for increased throughput and support specialization for tasks such as dedicated result collection.
  \item Clients: These submit tasks to the system.
\end{enumerate}

XtremWeb employs a two-part scheduling system:
\begin{enumerate}
  \item Dispatcher: Selects tasks from the pool based on application priorities.
  \item Scheduler: Assigns tasks to workers.
\end{enumerate}

The default scheduling policy is FIFO, but policies are dynamically configurable. The system supports timeout detection and rescheduling of aborted tasks.

One of the key features of XtremWeb is its worker-initiated communication protocol, which facilitates easier deployment through firewalls. The protocol consists of four main requests: hostRegister, workRequest, workAlive, and workResult. This protocol is independent of the communication layer, allowing for flexibility in implementation (e.g., TCP/UDP, CORBA, Java RMI). \cite{relatedwork:xtremweb}

Security in XtremWeb is addressed through native code execution for performance and an ad-hoc verification process for downloadable applications. Future versions plan to implement Software Fault Isolation for enhanced security.

XtremWeb has been successfully applied to various projects, including the Pierre Auger Observatory for studying high-energy cosmic rays. In this application, XtremWeb was used to run the \ac{AIRES} program by partitioning large simulations into smaller subtasks. \cite{relatedwork:xtremweb}

In summary, XtremWeb provides a flexible and robust platform for experimenting with Global Computing concepts. Its approach and concept is similar to WebCrowd, but the original project not maintained anymore since its development in the early 2000s.

\subsection{EGEE}
\label{subsec:background:related_work:egee}
Initiated on April 1, 2004, \ac{EGEE} was a two-year project within a planned four-year program, involving 71 partners from Europe, Russia, and the United States \cite{relatedwork:egee}. The project's primary objectives were to establish a seamless European grid infrastructure for scientific research, provide production-level grid services, and re-engineer grid middleware for enhanced robustness and scalability. \cite{relatedwork:egee}

The infrastructure of \ac{EGEE} was built upon the EU Research Network GEANT, leveraging expertise from previous initiatives such as EU DataGrid, UK e-Science, INFN Grid, Nordugrid, and US Trillium. The project aimed to expand its computational capabilities from an initial 3000 CPUs at 10 sites to 10000 CPUs across 50 sites by the end of its second year. \cite{relatedwork:egee}

The project focused on two primary pilot applications: 
\begin{itemize}
  \item The \ac{LCG} for high-energy physics data analysis.
  \item Biomedical Grids addressing challenges in genomic database mining and medical database indexing
\end{itemize}

A key aspect of EGEE was its emphasis on middleware development. The project focused on re-engineering existing middleware to address issues from first-generation implementations and ensure adherence to \ac{OGSA} standards. This approach aimed to enhance the reliability and scalability of the grid infrastructure. \cite{relatedwork:egee}

In conclusion, the \ac{EGEE} project represented a significant effort to transform grid computing from a research concept into a practical infrastructure to support "e-science" across Europe.

\subsection{WebAssembly for Edge Computing}
\label{subsec:background:related_work:wasmedgecomputing}
[WebAssembly for Edge Computing: Potential and Challenges] \cite{relatedwork:wasmedgecomputing}